{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone project - Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Can a predictive model be built for future prediction of the possibility of complaints of the specific type that you identified in response to Question 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Q3 I trained a Random Tree Forest classifier for identifying which are the most related features to certain complaint type-\n",
    "\n",
    "    The Random Tree Forest can also be used for predicting. We can use the test data for seeing how good it is as a classifier/predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "feature_list = ['BldgArea', 'BldgDepth', 'BuiltFAR', 'CommFAR', 'FacilFAR', 'Lot', 'LotArea', 'LotDepth', 'NumBldgs', 'NumFloors', 'OfficeArea', 'ResArea', 'ResidFAR', 'RetailArea', 'YearBuilt', 'YearAlter1']\n",
    "\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "client_dd7eba6d38744f2ba0e41e1f24c776e4 = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='***',\n",
    "    ibm_auth_endpoint=\"https://iam.eu-gb.bluemix.net/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3.eu-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "body0 = client_dd7eba6d38744f2ba0e41e1f24c776e4.get_object(Bucket='capstoneproject-donotdelete-pr-w1e8lgzafyovbk',Key='BK_18v1.csv')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body0, \"__iter__\"): body0.__iter__ = types.MethodType( __iter__, body0 )\n",
    "df_plutonBK = pd.read_csv(body0, usecols=['Address']+feature_list+['ZipCode', 'YCoord', 'XCoord'])\n",
    "\n",
    "body1 = client_dd7eba6d38744f2ba0e41e1f24c776e4.get_object(Bucket='capstoneproject-donotdelete-pr-w1e8lgzafyovbk',Key='MN_18v1.csv')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body1, \"__iter__\"): body1.__iter__ = types.MethodType( __iter__, body1 )\n",
    "df_plutonMN = pd.read_csv(body1, usecols=['Address']+feature_list+['ZipCode', 'YCoord', 'XCoord'])\n",
    "\n",
    "body2 = client_dd7eba6d38744f2ba0e41e1f24c776e4.get_object(Bucket='capstoneproject-donotdelete-pr-w1e8lgzafyovbk',Key='QN_18v1.csv')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body2, \"__iter__\"): body2.__iter__ = types.MethodType( __iter__, body2 )\n",
    "df_plutonQN = pd.read_csv(body2, usecols=['Address']+feature_list+['ZipCode', 'YCoord', 'XCoord'])\n",
    "\n",
    "body3 = client_dd7eba6d38744f2ba0e41e1f24c776e4.get_object(Bucket='capstoneproject-donotdelete-pr-w1e8lgzafyovbk',Key='SI_18v1.csv')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body3, \"__iter__\"): body3.__iter__ = types.MethodType( __iter__, body3 )\n",
    "df_plutonSI = pd.read_csv(body3, usecols=['Address']+feature_list+['ZipCode', 'YCoord', 'XCoord'])\n",
    "\n",
    "body4 = client_dd7eba6d38744f2ba0e41e1f24c776e4.get_object(Bucket='capstoneproject-donotdelete-pr-w1e8lgzafyovbk',Key='BX_18v1.csv')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body4, \"__iter__\"): body4.__iter__ = types.MethodType( __iter__, body4 )\n",
    "df_plutonBX = pd.read_csv(body4, usecols=['Address']+feature_list+['ZipCode', 'YCoord', 'XCoord'])\n",
    "\n",
    "df_pluton = df_plutonBK.append(df_plutonBX).append(df_plutonMN).append(df_plutonQN).append(df_plutonSI)\n",
    "\n",
    "body5 = client_dd7eba6d38744f2ba0e41e1f24c776e4.get_object(Bucket='capstoneproject-donotdelete-pr-w1e8lgzafyovbk',Key='fhrw-4uyv.csv')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body5, \"__iter__\"): body5.__iter__ = types.MethodType( __iter__, body5 )\n",
    "\n",
    "df_131 = pd.read_csv(body5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS PART COMES FROM Q3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Selecting the columns we are interested in\n",
    "df = df_131[['incident_address','complaint_type']].copy() \n",
    "#Cleaning the dataFrame\n",
    "indexDrop = df.loc[(df['complaint_type'] == 'HPD Literature Request')].index \n",
    "df.drop(indexDrop,inplace=True)\n",
    "# Dropping rows with empty cell\n",
    "df.dropna(inplace=True)\n",
    "# Dropping duplicated complaints for \n",
    "df.drop_duplicates(subset=['incident_address','complaint_type'],inplace=True)\n",
    "#Setting the index to incient_adrres so we can join the df_131 and df_pluton\n",
    "df.set_index('incident_address',inplace=True)\n",
    "df_131Pluton = df.join(other=df_pluton.set_index('Address'),how='inner')\n",
    "df_tree = df_131Pluton.loc[(df_131Pluton['complaint_type'] == 'GENERAL CONSTRUCTION') | ( df_131Pluton['complaint_type']=='PAINT - PLASTER') | ( df_131Pluton['complaint_type']=='PLUMBING')]\n",
    "\n",
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "features = df_tree[feature_list]\n",
    "labels =  df_tree['complaint_type']\n",
    "codes_labels, uniques_labels = pd.factorize(labels)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features,codes_labels, test_size = 0.25, random_state = 42)\n",
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.91 degrees.\n"
     ]
    }
   ],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
